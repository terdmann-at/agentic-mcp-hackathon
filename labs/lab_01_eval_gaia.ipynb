{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "264b4d61",
   "metadata": {},
   "source": [
    "# GAIA Evaluation\n",
    "\n",
    "This notebook evaluates the Deep Research Agent against the GAIA benchmark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a808627",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langgraph ddgs databricks-langchain pydantic pandas typing_extensions\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ab5e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from deep_research_app import app as research_agent\n",
    "from langchain.agents import create_agent\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "from llm import model as judge_llm\n",
    "from llm import model as llm\n",
    "\n",
    "react_agent = create_agent(llm, [DuckDuckGoSearchRun()])\n",
    "filtered_df = pd.read_csv(\"gaia_validation_level1.csv\")[:5]\n",
    "\n",
    "print(f\"Loaded {len(filtered_df)} tasks for evaluation.\")\n",
    "\n",
    "\n",
    "# 3. Define Judge Wrapper\n",
    "def query_judge_model(question, predicted, truth, metadata):\n",
    "    \"\"\"\n",
    "    Evaluates the answer using the Judge LLM.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an impartial judge.\n",
    "\n",
    "    [CONTEXT/METADATA]: {metadata}\n",
    "    [QUESTION]: {question}\n",
    "    [GROUND TRUTH]: {truth}\n",
    "    [PREDICTED]: {predicted}\n",
    "\n",
    "    Compare Predicted to Ground Truth. Assign a score 1-10.\n",
    "    1 = Wrong, 10 = Perfect.\n",
    "    Also provide a short explanation.\n",
    "\n",
    "    Output format:\n",
    "    SCORE: [Score]\n",
    "    REASON: [Short explanation]\n",
    "    \"\"\"\n",
    "    return judge_llm.invoke(prompt).content\n",
    "\n",
    "\n",
    "def extract_score(judge_response):\n",
    "    match = re.search(r\"SCORE:\\s*(\\d+)\", judge_response)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "\n",
    "# 4. Run Evaluation Loop\n",
    "results = []\n",
    "for index, row in filtered_df.iterrows():\n",
    "    task_id = row[\"task_id\"]\n",
    "    question = row[\"Question\"]\n",
    "    truth = row[\"Final answer\"]\n",
    "    metadata = str(row[\"Annotator Metadata\"])\n",
    "\n",
    "    print(f\"\\nProcessing Task: {task_id}\")\n",
    "\n",
    "    # --- Agent 1: Deep Research ---\n",
    "    result = research_agent.invoke({\"topic\": question})\n",
    "    predicted_dr = result.get(\"final_report\")\n",
    "    print(f\"[Deep Research Output]: {predicted_dr[:100]}...\")\n",
    "\n",
    "    judge_resp_dr = query_judge_model(question, predicted_dr, truth, metadata)\n",
    "    score_dr = extract_score(judge_resp_dr)\n",
    "    print(f\"Deep Research Score: {score_dr}\")\n",
    "\n",
    "    # --- Agent 2: ReAct Baseline ---\n",
    "    print(\"[ReAct] Researching...\")\n",
    "    result = react_agent.invoke({\"messages\": [question]})\n",
    "    predicted_react = result[\"messages\"][-1].content\n",
    "    print(f\"[ReAct Output]: {predicted_react[:100]}...\")\n",
    "\n",
    "    judge_resp_react = query_judge_model(question, predicted_react, truth, metadata)\n",
    "    score_react = extract_score(judge_resp_react)\n",
    "    print(f\"ReAct Score: {score_react}\")\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"task_id\": task_id,\n",
    "            \"question\": question,\n",
    "            \"ground_truth\": truth,\n",
    "            \"deep_research_pred\": predicted_dr,\n",
    "            \"deep_research_score\": score_dr,\n",
    "            \"react_pred\": predicted_react,\n",
    "            \"react_score\": score_react,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# 5. Summary\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "print(results_df)\n",
    "\n",
    "# Optional: Save to CSV\n",
    "# results_df.to_csv(\"gaia_eval_results_pandas.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
