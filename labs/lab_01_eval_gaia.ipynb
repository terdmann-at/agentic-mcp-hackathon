{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2077cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec93bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the configured model (Judge) and the Agent Application (Solver)\n",
    "from llm import model as judge_llm\n",
    "from lab_01_deep_research import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac797eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load GAIA Validation Set\n",
    "print(\"Loading GAIA dataset...\")\n",
    "data_dir = snapshot_download(repo_id=\"gaia-benchmark/GAIA\", repo_type=\"dataset\")\n",
    "dataset = load_dataset(data_dir, \"2023_level1\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0f0afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas\n",
    "df = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718e0792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataset (exclude multimedia tools and file uploads for this text-only agent)\n",
    "# Conditions:\n",
    "# A. Annotator Metadata does NOT contain video/image/youtube\n",
    "# B. file_name is empty or null\n",
    "mask_no_multimedia = ~df[\"Annotator Metadata\"].astype(str).str.lower().str.contains(\"video|image|youtube\", regex=True)\n",
    "mask_no_file = df[\"file_name\"].isnull() | (df[\"file_name\"] == \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac714fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[mask_no_multimedia & mask_no_file].head(5).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d78ef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loaded {len(filtered_df)} tasks for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d7ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define Solver Wrapper\n",
    "def query_solver_model(question):\n",
    "    \"\"\"\n",
    "    Invokes the Deep Research Agent.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[Solver] Researching: {question[:50]}...\")\n",
    "    try:\n",
    "        # invoke the graph with the question\n",
    "        result = app.invoke({\"topic\": question})\n",
    "        return result.get(\"final_report\", \"No report generated.\")\n",
    "    except Exception as e:\n",
    "        return f\"Error during research: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0774b8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define Judge Wrapper\n",
    "def query_judge_model(question, predicted, truth, metadata):\n",
    "    \"\"\"\n",
    "    Evaluates the answer using the Judge LLM.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an impartial judge.\n",
    "\n",
    "    [CONTEXT/METADATA]: {metadata}\n",
    "    [QUESTION]: {question}\n",
    "    [GROUND TRUTH]: {truth}\n",
    "    [PREDICTED]: {predicted}\n",
    "\n",
    "    Compare Predicted to Ground Truth. Assign a score 1-10.\n",
    "    1 = Wrong, 10 = Perfect.\n",
    "    Also provide a short explanation.\n",
    "\n",
    "    Output format:\n",
    "    SCORE: [Score]\n",
    "    REASON: [Short explanation]\n",
    "    \"\"\"\n",
    "    return judge_llm.invoke(prompt).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cc5ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_score(judge_response):\n",
    "    match = re.search(r\"SCORE:\\s*(\\d+)\", judge_response)\n",
    "    return int(match.group(1)) if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e189b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Run Evaluation Loop\n",
    "# Import the ReAct agent\n",
    "from lab_01_react import run_react_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecee458e",
   "metadata": {},
   "source": [
    "... (Previous imports remain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1898e345",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# 4. Run Evaluation Loop\n",
    "results = []\n",
    "for index, row in filtered_df.iterrows():\n",
    "    task_id = row[\"task_id\"]\n",
    "    question = row[\"Question\"]\n",
    "    truth = row[\"Final answer\"]\n",
    "    metadata = str(row[\"Annotator Metadata\"])\n",
    "\n",
    "    print(f\"\\nProcessing Task: {task_id}\")\n",
    "\n",
    "    # --- Agent 1: Deep Research ---\n",
    "    predicted_dr = query_solver_model(question)\n",
    "    print(f\"[Deep Research Output]: {predicted_dr[:100]}...\")\n",
    "    \n",
    "    judge_resp_dr = query_judge_model(question, predicted_dr, truth, metadata)\n",
    "    score_dr = extract_score(judge_resp_dr)\n",
    "    print(f\"Deep Research Score: {score_dr}\")\n",
    "\n",
    "    # --- Agent 2: ReAct Baseline ---\n",
    "    print(f\"[ReAct] Researching...\")\n",
    "    predicted_react = run_react_agent(question)\n",
    "    print(f\"[ReAct Output]: {predicted_react[:100]}...\")\n",
    "\n",
    "    judge_resp_react = query_judge_model(question, predicted_react, truth, metadata)\n",
    "    score_react = extract_score(judge_resp_react)\n",
    "    print(f\"ReAct Score: {score_react}\")\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"task_id\": task_id,\n",
    "            \"question\": question,\n",
    "            \"ground_truth\": truth,\n",
    "            \"deep_research_pred\": predicted_dr,\n",
    "            \"deep_research_score\": score_dr,\n",
    "            \"react_pred\": predicted_react,\n",
    "            \"react_score\": score_react\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7166da9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Summary\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5446fde",
   "metadata": {},
   "source": [
    "Optional: Save to CSV\n",
    "results_df.to_csv(\"gaia_eval_results_pandas.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
