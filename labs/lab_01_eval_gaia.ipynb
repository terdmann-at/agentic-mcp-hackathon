{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "305a9836",
   "metadata": {},
   "source": [
    "# GAIA Evaluation\n",
    "\n",
    "This notebook evaluates the Deep Research Agent against the GAIA benchmark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a63b0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langgraph duckduckgo-search databricks-langchain pydantic pandas polars datasets huggingface_hub typing_extensions\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f43fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Import the configured model (Judge) and the Agent Application (Solver)\n",
    "# Ensure src is in python path if running from root\n",
    "import sys\n",
    "if \"src\" not in sys.path:\n",
    "    sys.path.append(\"src\")\n",
    "\n",
    "from llm import model as judge_llm\n",
    "from lab_01_deep_research import app\n",
    "\n",
    "# 1. Load GAIA Validation Set\n",
    "print(\"Loading GAIA dataset...\")\n",
    "CSV_FILE = \"gaia_validation_level1.csv\"\n",
    "\n",
    "if os.path.exists(CSV_FILE):\n",
    "    print(f\"Loading dataset from {CSV_FILE}...\")\n",
    "    df = pd.read_csv(CSV_FILE)\n",
    "else:\n",
    "    print(\"Downloading dataset from HuggingFace...\")\n",
    "    data_dir = snapshot_download(repo_id=\"gaia-benchmark/GAIA\", repo_type=\"dataset\")\n",
    "    dataset = load_dataset(data_dir, \"2023_level1\", split=\"validation\")\n",
    "    \n",
    "    # Convert to Pandas\n",
    "    df = dataset.to_pandas()\n",
    "    \n",
    "    # Save to CSV for next time\n",
    "    print(f\"Saving dataset to {CSV_FILE}...\")\n",
    "    df.to_csv(CSV_FILE, index=False)\n",
    "\n",
    "\n",
    "# Filter dataset (exclude multimedia tools and file uploads for this text-only agent)\n",
    "# Conditions:\n",
    "# A. Annotator Metadata does NOT contain video/image/youtube\n",
    "# B. file_name is empty or null\n",
    "mask_no_multimedia = ~df[\"Annotator Metadata\"].astype(str).str.lower().str.contains(\"video|image|youtube\", regex=True)\n",
    "mask_no_file = df[\"file_name\"].isnull() | (df[\"file_name\"] == \"\")\n",
    "\n",
    "filtered_df = df[mask_no_multimedia & mask_no_file].head(5).copy()\n",
    "\n",
    "print(f\"Loaded {len(filtered_df)} tasks for evaluation.\")\n",
    "\n",
    "\n",
    "# 2. Define Solver Wrapper\n",
    "def query_solver_model(question):\n",
    "    \"\"\"\n",
    "    Invokes the Deep Research Agent.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[Solver] Researching: {question[:50]}...\")\n",
    "    try:\n",
    "        # invoke the graph with the question\n",
    "        result = app.invoke({\"topic\": question})\n",
    "        return result.get(\"final_report\", \"No report generated.\")\n",
    "    except Exception as e:\n",
    "        return f\"Error during research: {str(e)}\"\n",
    "\n",
    "\n",
    "# 3. Define Judge Wrapper\n",
    "def query_judge_model(question, predicted, truth, metadata):\n",
    "    \"\"\"\n",
    "    Evaluates the answer using the Judge LLM.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an impartial judge.\n",
    "\n",
    "    [CONTEXT/METADATA]: {metadata}\n",
    "    [QUESTION]: {question}\n",
    "    [GROUND TRUTH]: {truth}\n",
    "    [PREDICTED]: {predicted}\n",
    "\n",
    "    Compare Predicted to Ground Truth. Assign a score 1-10.\n",
    "    1 = Wrong, 10 = Perfect.\n",
    "    Also provide a short explanation.\n",
    "\n",
    "    Output format:\n",
    "    SCORE: [Score]\n",
    "    REASON: [Short explanation]\n",
    "    \"\"\"\n",
    "    return judge_llm.invoke(prompt).content\n",
    "\n",
    "\n",
    "def extract_score(judge_response):\n",
    "    match = re.search(r\"SCORE:\\s*(\\d+)\", judge_response)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "\n",
    "# 4. Run Evaluation Loop\n",
    "# Import the ReAct agent\n",
    "from lab_01_react import run_react_agent\n",
    "\n",
    "# ... (Previous imports remain)\n",
    "\n",
    "# 4. Run Evaluation Loop\n",
    "results = []\n",
    "for index, row in filtered_df.iterrows():\n",
    "    task_id = row[\"task_id\"]\n",
    "    question = row[\"Question\"]\n",
    "    truth = row[\"Final answer\"]\n",
    "    metadata = str(row[\"Annotator Metadata\"])\n",
    "\n",
    "    print(f\"\\nProcessing Task: {task_id}\")\n",
    "\n",
    "    # --- Agent 1: Deep Research ---\n",
    "    predicted_dr = query_solver_model(question)\n",
    "    print(f\"[Deep Research Output]: {predicted_dr[:100]}...\")\n",
    "    \n",
    "    judge_resp_dr = query_judge_model(question, predicted_dr, truth, metadata)\n",
    "    score_dr = extract_score(judge_resp_dr)\n",
    "    print(f\"Deep Research Score: {score_dr}\")\n",
    "\n",
    "    # --- Agent 2: ReAct Baseline ---\n",
    "    print(f\"[ReAct] Researching...\")\n",
    "    predicted_react = run_react_agent(question)\n",
    "    print(f\"[ReAct Output]: {predicted_react[:100]}...\")\n",
    "\n",
    "    judge_resp_react = query_judge_model(question, predicted_react, truth, metadata)\n",
    "    score_react = extract_score(judge_resp_react)\n",
    "    print(f\"ReAct Score: {score_react}\")\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"task_id\": task_id,\n",
    "            \"question\": question,\n",
    "            \"ground_truth\": truth,\n",
    "            \"deep_research_pred\": predicted_dr,\n",
    "            \"deep_research_score\": score_dr,\n",
    "            \"react_pred\": predicted_react,\n",
    "            \"react_score\": score_react\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# 5. Summary\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "print(results_df)\n",
    "\n",
    "# Optional: Save to CSV\n",
    "# results_df.to_csv(\"gaia_eval_results_pandas.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
