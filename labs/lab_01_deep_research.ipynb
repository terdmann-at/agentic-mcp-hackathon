{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c96238e",
   "metadata": {},
   "source": [
    "# Lab 1: Deep Research Agent with Multi-Agent System\n",
    "\n",
    "In this lab, we will build a \"Deep Research\" agent.\n",
    "This agent will take a topic, break it down into sub-topics, research them in parallel, and compile a final report.\n",
    "\n",
    "We will use `LangGraph` for orchestration and `Databricks` Model Serving for the LLM.\n",
    "\n",
    "## Structure\n",
    "1. **State Definition**: define the data structure that flows through the graph.\n",
    "2. **Nodes**: define the agents (Chief Editor, Researcher, Writer).\n",
    "3. **Graph**: connect the nodes.\n",
    "4. **Human-in-the-Loop**: add a planning phase with interrupts.\n",
    "\n",
    "First, let's install the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc92bdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langgraph ddgs databricks-langchain pydantic typing_extensions\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdd8182",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Import necessary libraries and initialize the LLM and Tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c34547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, TypedDict\n",
    "\n",
    "try:\n",
    "    from typing import NotRequired\n",
    "except ImportError:\n",
    "    from typing_extensions import NotRequired\n",
    "\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.types import Command, Send, interrupt\n",
    "\n",
    "# Initialize Model\n",
    "from llm import model as llm\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Initialize Search Tool\n",
    "search_tool = DuckDuckGoSearchRun()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94870b75",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Exercise 1: State Definition\n",
    "\n",
    "We need to define the state that holds our research data.\n",
    "\n",
    "The `ResearchState` should track:\n",
    "- `topic`: The user's original query.\n",
    "- `sub_topics`: A list of strings for parallel research.\n",
    "- `research_outputs`: A list of results from workers (this needs to be cumulative!).\n",
    "- `final_report`: The generated output.\n",
    "\n",
    "The `SubTaskState` is for individual workers and needs:\n",
    "- `topic`: The specific sub-topic to research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168cecb6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Exercise 1: Define the State classes. Use either pydantic classes or `TypedDict`\n",
    "# <solution>\n",
    "class SubTaskState(TypedDict):\n",
    "    \"\"\"State for a single research worker agent.\"\"\"\n",
    "\n",
    "    topic: str\n",
    "    result: str\n",
    "\n",
    "\n",
    "class ResearchState(TypedDict):\n",
    "    \"\"\"Global state for the entire graph.\"\"\"\n",
    "\n",
    "    topic: str\n",
    "    sub_topics: List[str]\n",
    "    # Use operator.add to append new outputs to the list instead of overwriting\n",
    "    research_outputs: Annotated[List[str], operator.add]\n",
    "    final_report: str\n",
    "\n",
    "\n",
    "# </solution>\n",
    "\n",
    "\n",
    "class ResearchPlan(BaseModel):\n",
    "    \"\"\"Structured output for the Chief Editor.\"\"\"\n",
    "\n",
    "    sub_topics: List[str] = Field(\n",
    "        description=\"List of 3 distinct sub-topics to research in parallel\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35577d2d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Exercise 2: Define Nodes\n",
    "\n",
    "We need three nodes:\n",
    "\n",
    "1. **Chief Editor**: breaks the topic into sub-topics.\n",
    "2. **Research Worker**: searches for information on a sub-topic.\n",
    "3. **Writer**: compiles the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99d452a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def chief_editor_node(state: ResearchState):\n",
    "    print(f\"--- [Chief Editor] Planning: {state['topic']} ---\")\n",
    "\n",
    "    # Exercise 2.1: Implement the Chief Editor\n",
    "    # Use the LLM to generate a 'ResearchPlan' from the topic.\n",
    "    # Hint: have a look at https://docs.langchain.com/oss/python/langchain/structured-output\n",
    "    # <solution>\n",
    "    planner = llm.with_structured_output(ResearchPlan)\n",
    "    prompt = (\n",
    "        f\"You are a Research Manager. Your goal is to break down the following research topic into 3 distinct, \"\n",
    "        f\"targeted sub-topics that will convince a search engine to reveal specific facts, numbers, or data points.\\n\\n\"\n",
    "        f\"Topic: {state['topic']}\\n\\n\"\n",
    "        f\"Return 3 distinct sub-topics.\"\n",
    "    )\n",
    "    plan = planner.invoke(prompt)\n",
    "\n",
    "    return {\"sub_topics\": plan.sub_topics}\n",
    "    # </solution>\n",
    "\n",
    "\n",
    "def research_worker_node(state: SubTaskState):\n",
    "    topic = state[\"topic\"]\n",
    "    print(f\"--- [Worker] Searching for: {topic} ---\")\n",
    "\n",
    "    # Exercise 2.2: Implement the Research Worker\n",
    "    # Use 'search_tool' to find info and return it in 'research_outputs'.\n",
    "    # <solution>\n",
    "    try:\n",
    "        res = search_tool.invoke(topic)\n",
    "    except Exception as e:\n",
    "        res = f\"Search failed: {e}\"\n",
    "\n",
    "    return {\"research_outputs\": [f\"## Subtopic: {topic}\\n{res}\\n\"]}\n",
    "    # </solution>\n",
    "\n",
    "\n",
    "def writer_node(state: ResearchState):\n",
    "    print(\"--- [Writer] Compiling Report ---\")\n",
    "\n",
    "    # Exercise 2.3: Implement the Writer\n",
    "    # Combine 'research_outputs' and ask the LLM to write a report.\n",
    "    # <solution>\n",
    "    combined_content = \"\\n\\n\".join(state[\"research_outputs\"])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a technical writer. Compile the following research notes into a comprehensive final report.\n",
    "\n",
    "    Topic: {state[\"topic\"]}\n",
    "\n",
    "    Research Notes:\n",
    "    {combined_content}\n",
    "\n",
    "    Instructions:\n",
    "    1. Synthesize the information into a clear, well-structured report.\n",
    "    2. End with a \"Final Answer:\" section.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"final_report\": response.content}\n",
    "    # </solution>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f6df3c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Exercise 3: Graph Construction\n",
    "\n",
    "Now we wire them together.\n",
    "\n",
    "- **Start** -> **Chief Editor**\n",
    "- **Chief Editor** -> **Workers** (Conditional Edge using `Send`)\n",
    "- **Workers** -> **Writer**\n",
    "- **Writer** -> **End**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3e8e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_subtopics(state: ResearchState):\n",
    "    # Exercise 3.1: Define the mapping logic\n",
    "    # Return a list of `Send` objects, one for each sub-topic.\n",
    "    # <solution>\n",
    "    return [\n",
    "        Send(\"research_worker\", {\"topic\": sub_topic})\n",
    "        for sub_topic in state[\"sub_topics\"]\n",
    "    ]\n",
    "    # </solution>\n",
    "\n",
    "\n",
    "# Exercise 3.2: Build the Graph\n",
    "# <solution>\n",
    "workflow = StateGraph(ResearchState)\n",
    "\n",
    "workflow.add_node(\"chief_editor\", chief_editor_node)\n",
    "workflow.add_node(\"research_worker\", research_worker_node)\n",
    "workflow.add_node(\"writer\", writer_node)\n",
    "\n",
    "workflow.add_edge(START, \"chief_editor\")\n",
    "workflow.add_conditional_edges(\"chief_editor\", map_subtopics, [\"research_worker\"])\n",
    "workflow.add_edge(\"research_worker\", \"writer\")\n",
    "workflow.add_edge(\"writer\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "# </solution>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c359200a",
   "metadata": {},
   "source": [
    "### Run the Graph (Basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b9ed5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = app.invoke({\"topic\": \"The future of Agentic AI\"})\n",
    "print(res[\"final_report\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4537ff00",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Exercise 4: Human-in-the-Loop (Planning Phase)\n",
    "\n",
    "Real-world agents need supervision. Let's add a \"Planning Phase\" where the user can review and edit the sub-topics before research begins.\n",
    "\n",
    "We will implement a cycle in the graph:\n",
    "1. **Planner**: generates an initial plan (or regenerates based on feedback).\n",
    "2. **Reviewer**: interrupts execution to request user approval.\n",
    "3. **Conditional Edge**:\n",
    "   - If approved -> proceed to **Research Workers**.\n",
    "   - If rejected (with critique) -> loop back to **Planner**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505eda4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new state for HITL\n",
    "class ResearchStateHITL(ResearchState):\n",
    "    critique: NotRequired[str]\n",
    "    approved: NotRequired[bool]\n",
    "\n",
    "\n",
    "def planner_node(state: ResearchStateHITL):\n",
    "    print(f\"--- [Planner] Planning: {state['topic']} ---\")\n",
    "\n",
    "    # <solution>\n",
    "    planner = llm.with_structured_output(ResearchPlan)\n",
    "\n",
    "    # If there is a critique, we are regenerating\n",
    "    if state.get(\"critique\"):\n",
    "        print(f\"--- [Planner] Regenerating with critique: {state.get('critique')} ---\")\n",
    "        prompt = f\"\"\"\n",
    "        Original Topic: {state[\"topic\"]}\n",
    "        Previous Plan: {state.get(\"sub_topics\")}\n",
    "        User Critique: {state.get(\"critique\")}\n",
    "\n",
    "        Generate a new plan with 3 distinct sub-topics that addresses the critique.\n",
    "        \"\"\"\n",
    "    else:\n",
    "        # Initial plan\n",
    "        prompt = f\"Topic: {state['topic']}\\nReturn 3 distinct sub-topics.\"\n",
    "    # </solution>\n",
    "\n",
    "    plan = planner.invoke(prompt)\n",
    "    return {\"sub_topics\": plan.sub_topics, \"approved\": False}\n",
    "\n",
    "\n",
    "def reviewer_node(state: ResearchStateHITL):\n",
    "    # Exercise 4: Add the interrupt\n",
    "    # <solution>\n",
    "    # Interrupt and wait for feedback\n",
    "    # We allow the user to provide {\"approved\": True} or {\"critique\": \"...\"}\n",
    "    feedback = interrupt(\n",
    "        {\n",
    "            \"sub_topics\": state[\"sub_topics\"],\n",
    "            \"message\": \"Please review the research plan. Provide 'approved': True or 'critique': str.\",\n",
    "        }\n",
    "    )\n",
    "    # The feedback from resume is expected to update the state\n",
    "    return feedback\n",
    "    # </solution>\n",
    "\n",
    "\n",
    "def should_continue(state: ResearchStateHITL):\n",
    "    \"\"\"\n",
    "    Conditional edge logic:\n",
    "    - If approved -> Map to research workers\n",
    "    - If not approved -> Loop back to planner\n",
    "    \"\"\"\n",
    "    if state.get(\"approved\"):\n",
    "        # Map to workers\n",
    "        return [\n",
    "            Send(\"research_worker\", {\"topic\": sub_topic})\n",
    "            for sub_topic in state[\"sub_topics\"]\n",
    "        ]\n",
    "\n",
    "    # Loop back\n",
    "    return \"planner\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e26abc",
   "metadata": {},
   "source": [
    "### Re-build the Graph with HITL\n",
    "The logic is now explicit in the graph structure:\n",
    "`Planner` -> `Reviewer` -> (conditional) -> `Planner` or `Workers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb74cb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# Re-define graph\n",
    "workflow_hitl = StateGraph(ResearchStateHITL)\n",
    "\n",
    "workflow_hitl.add_node(\"planner\", planner_node)\n",
    "workflow_hitl.add_node(\"reviewer\", reviewer_node)\n",
    "workflow_hitl.add_node(\"research_worker\", research_worker_node)\n",
    "workflow_hitl.add_node(\"writer\", writer_node)\n",
    "\n",
    "# Start -> Planner\n",
    "workflow_hitl.add_edge(START, \"planner\")\n",
    "\n",
    "# Planner -> Reviewer\n",
    "workflow_hitl.add_edge(\"planner\", \"reviewer\")\n",
    "\n",
    "# Reviewer -> Conditional (Planner or Workers)\n",
    "workflow_hitl.add_conditional_edges(\n",
    "    \"reviewer\", should_continue, [\"planner\", \"research_worker\"]\n",
    ")\n",
    "\n",
    "workflow_hitl.add_edge(\"research_worker\", \"writer\")\n",
    "workflow_hitl.add_edge(\"writer\", END)\n",
    "\n",
    "# Compile with checkpointer\n",
    "checkpointer = InMemorySaver()\n",
    "app_hitl = workflow_hitl.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fbb1ff",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Run Interactive Session\n",
    "\n",
    "We need to handle the execution flow:\n",
    "1. Run until interrupt.\n",
    "2. Inspect payload.\n",
    "3. Resume with feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c395d453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_research_interactive():\n",
    "    \"\"\"\n",
    "    Runs the research agent in an interactive loop.\n",
    "    Allows the user to review the plan and provide feedback.\n",
    "    \"\"\"\n",
    "    # 1. Setup\n",
    "    thread_id = \"research-thread-interact\"\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "    print(\"Welcome to the Deep Research Agent!\")\n",
    "    topic = input(\"Enter a research topic: \")\n",
    "\n",
    "    # We use a loop to handle the stream and inputs\n",
    "    # Initial input is the topic\n",
    "    current_input = {\"topic\": topic}\n",
    "    resume_command = None\n",
    "\n",
    "    while True:\n",
    "        # If we have a resume command (from the 2nd loop onwards), use it\n",
    "        if resume_command:\n",
    "            stream_input = resume_command\n",
    "        else:\n",
    "            stream_input = current_input\n",
    "\n",
    "        # Run the graph until it interrupts or finishes\n",
    "        # We need to stream to capture interrupts\n",
    "        events = app_hitl.stream(stream_input, config=config)\n",
    "\n",
    "        current_interrupt_value = None\n",
    "        final_report = None\n",
    "\n",
    "        print(\"\\n--- Agent Working ---\")\n",
    "        for event in events:\n",
    "            # Check for interrupt\n",
    "            if \"__interrupt__\" in event:\n",
    "                current_interrupt_value = event[\"__interrupt__\"][0].value\n",
    "\n",
    "            # Check for writer output (final step)\n",
    "            if \"writer\" in event:\n",
    "                final_report = event[\"writer\"].get(\"final_report\")\n",
    "\n",
    "        # A. If we got a final report, we are done\n",
    "        if final_report:\n",
    "            print(\"\\n\" + \"=\" * 40)\n",
    "            print(\"FINAL REPORT\")\n",
    "            print(\"=\" * 40)\n",
    "            print(final_report)\n",
    "            break\n",
    "\n",
    "        # B. If we hit an interrupt, ask for user feedback\n",
    "        if current_interrupt_value:\n",
    "            print(\"\\n\" + \"-\" * 40)\n",
    "            print(\"REVIEW PLAN\")\n",
    "            print(\"-\" * 40)\n",
    "            print(f\"Proposed Sub-topics: {current_interrupt_value['sub_topics']}\")\n",
    "\n",
    "            user_response = input(\n",
    "                \"\\nType 'ok' to approve, or enter your critique/changes: \"\n",
    "            ).strip()\n",
    "\n",
    "            if user_response.lower() in [\"ok\", \"yes\", \"approve\"]:\n",
    "                print(\"\\n> Approved. Proceeding to research...\")\n",
    "                resume_command = Command(resume={\"approved\": True})\n",
    "            else:\n",
    "                print(\"\\n> Critique received. Regenerating plan...\")\n",
    "                resume_command = Command(\n",
    "                    resume={\"approved\": False, \"critique\": user_response}\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa62c68e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Run the interactive session\n",
    "run_research_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5707f3e1",
   "metadata": {},
   "source": [
    "## Exercise 5 (Bonus): Benchmark Evaluation (GAIA)\n",
    "\n",
    "We will now evaluate our agent against some questions from the GAIA benchmark.\n",
    "We compare the Deep Research Agent against a standard ReAct baseline.\n",
    "\n",
    "Your task is to play with the prompt, or the structure of the system to improve the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b78575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from ddgs import DDGS\n",
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def web_search(query: str, max_results: int = 5):\n",
    "    \"\"\"Run a web search\"\"\"\n",
    "    return str(DDGS().text(query, max_results=max_results))\n",
    "\n",
    "\n",
    "# 1. Setup ReAct Baseline\n",
    "agent_react = create_agent(llm, [web_search])\n",
    "\n",
    "# 2. Load Dataset\n",
    "# Ensure create_gaia_dataset() has been run or the file exists.\n",
    "csv_path = \"gaia_validation_level1.csv\"\n",
    "try:\n",
    "    filtered_df = pd.read_csv(csv_path)[:2]\n",
    "    print(f\"Loaded {len(filtered_df)} tasks for evaluation.\")\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        f\"Dataset not found at {csv_path}. Please run 'python src/create_gaia.py' inside labs/ directory.\"\n",
    "    )\n",
    "    filtered_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "# 3. Define Judge\n",
    "def query_judge_model(question, predicted, truth, metadata):\n",
    "    prompt = f\"\"\"\n",
    "    You are an impartial judge.\n",
    "\n",
    "    [CONTEXT/METADATA]: {metadata}\n",
    "    [QUESTION]: {question}\n",
    "    [GROUND TRUTH]: {truth}\n",
    "    [PREDICTED]: {predicted}\n",
    "\n",
    "    Compare Predicted to Ground Truth. Assign a score 1-10.\n",
    "    1 = Wrong, 10 = Perfect.\n",
    "    Also provide a short explanation.\n",
    "\n",
    "    Output format:\n",
    "    SCORE: [Score]\n",
    "    REASON: [Short explanation]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return llm.invoke(prompt).content\n",
    "    except Exception as e:\n",
    "        return f\"SCORE: 0 REASON: Error calling judge: {e}\"\n",
    "\n",
    "\n",
    "def extract_score(judge_response):\n",
    "    match = re.search(r\"SCORE:\\s*(\\d+)\", judge_response)\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "\n",
    "# 4. Evaluation Loop\n",
    "results = []\n",
    "if not filtered_df.empty:\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        task_id = row[\"task_id\"]\n",
    "        question = row[\"Question\"]\n",
    "        truth = row[\"Final answer\"]\n",
    "        metadata = str(row[\"Annotator Metadata\"])\n",
    "\n",
    "        print(f\"\\nProcessing Task: {task_id}\")\n",
    "\n",
    "        # --- Agent 1: Deep Research ---\n",
    "        result_dr = app.invoke({\"topic\": question})\n",
    "        predicted_dr = result_dr.get(\"final_report\", \"No report generated.\")\n",
    "        judge_resp_dr = query_judge_model(question, predicted_dr, truth, metadata)\n",
    "        score_dr = extract_score(judge_resp_dr)\n",
    "        print(f\"Deep Research Score: {score_dr}\")\n",
    "\n",
    "        # --- Agent 2: ReAct Baseline ---\n",
    "        result_react = agent_react.invoke({\"messages\": [question]})\n",
    "        predicted_react = result_react[\"messages\"][-1].content\n",
    "        judge_resp_react = query_judge_model(question, predicted_react, truth, metadata)\n",
    "        score_react = extract_score(judge_resp_react)\n",
    "        print(f\"ReAct Score: {score_react}\")\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"task_id\": task_id,\n",
    "                \"question\": question,\n",
    "                \"ground_truth\": truth,\n",
    "                \"deep_research_pred\": predicted_dr,\n",
    "                \"deep_research_score\": score_dr,\n",
    "                \"react_pred\": predicted_react,\n",
    "                \"react_score\": score_react,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # 5. Results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\n=== Evaluation Results ===\")\n",
    "    print(results_df)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
