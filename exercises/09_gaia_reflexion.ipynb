{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afe6d12e",
   "metadata": {},
   "source": [
    "# Exercise 9: GAIA Question with Reflexion\n",
    "\n",
    "Goal: Solve a hard GAIA benchmark question using a Reflexion (ReAct + Critique) loop.\n",
    "\n",
    "GAIA Question:\n",
    "\"Which of the fruits shown in the 2008 painting 'Embroidery from Uzbekistan' were served\n",
    "as part of the October 1949 breakfast menu for the ocean liner that was later used as a\n",
    "floating prop for the film 'The Last Voyage'? Give the items as a comma-separated list...\"\n",
    "\n",
    "This requires:\n",
    "1. Understanding the question (Multi-hop reasoning).\n",
    "2. Tool use (Search) to find the painting and the ship's menu.\n",
    "3. Critique/Reflexion to ensure all constraints are met (e.g., \"plural form\", \"comma-separated\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004a24a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install databricks-langchain langchain-community duckduckgo-search\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097561d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks_langchain import ChatDatabricks\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "# 1. Setup Wrapper for easier handling\n",
    "# We'll use a simple Agent wrapper to run the search\n",
    "model = ChatDatabricks(endpoint=\"databricks-claude-sonnet-4-5\")\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "tools = [search_tool]\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "agent = create_react_agent(model, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23252b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. The GAIA Question\n",
    "gaia_question = \"\"\"\n",
    "Which of the fruits shown in the 2008 painting \"Embroidery from Uzbekistan\" were served as part of the October 1949 breakfast menu for the ocean liner that was later used as a floating prop for the film \"The Last Voyage\"? \n",
    "Give the items as a comma-separated list, ordering them in clockwise order based on their arrangement in the painting starting from the 12 o'clock position. Use the plural form of each fruit.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"--- GAIA Question ---\\n{gaia_question}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a017a6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. First Attempt (Actor)\n",
    "print(\"--- Attempt 1: Researching ---\")\n",
    "# We start with a plan or just run the agent.\n",
    "try:\n",
    "    initial_response = agent_executor.invoke({\"input\": gaia_question})\n",
    "    initial_answer = initial_response[\"output\"]\n",
    "except Exception as e:\n",
    "    initial_answer = f\"Error during execution: {e}\"\n",
    "\n",
    "print(f\"\\nInitial Answer:\\n{initial_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31e7c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 9.1: Critique (Reflector) & Loop\n",
    "#\n",
    "# Implement the critique step.\n",
    "# If failed, feed the critique back to the agent as \"Context\" to try again.\n",
    "\n",
    "# <solution>\n",
    "# 4. Critique (Reflector)\n",
    "# usage: Criticize the answer based on the requirements.\n",
    "critique_prompt = f\"\"\"\n",
    "You are a strict Evaluator. Verify if the following answer meets ALL the constraints of the question.\n",
    "\n",
    "Question: {gaia_question}\n",
    "\n",
    "Answer: {initial_answer}\n",
    "\n",
    "Checklist:\n",
    "1. Did it identify the painting \"Embroidery from Uzbekistan\"?\n",
    "2. Did it identify the ocean liner from \"The Last Voyage\"?\n",
    "3. Did it find the Oct 1949 breakfast menu?\n",
    "4. Is the list comma-separated?\n",
    "5. Is the order clockwise from 12 o'clock?\n",
    "6. Are fruits in plural form?\n",
    "\n",
    "Output your critique. If it is perfect, end with \"STATUS: PASS\". If not, provide specific instructions to fix it and end with \"STATUS: FAIL\".\n",
    "\"\"\"\n",
    "\n",
    "critic_response = model.invoke(critique_prompt).content\n",
    "print(f\"\\n--- Critique ---\\n{critic_response}\")\n",
    "\n",
    "# 5. Iterative Improvement (Loop)\n",
    "# If failed, we feed the critique back to the agent as \"Context\" to try again.\n",
    "\n",
    "if \"STATUS: FAIL\" in critic_response:\n",
    "    print(\"\\n--- Attempt 2: Refining based on critique ---\")\n",
    "\n",
    "    # We construct a new prompt for the agent including the history\n",
    "    retry_prompt = f\"\"\"\n",
    "    Previous Attempt Answer: {initial_answer}\n",
    "    \n",
    "    Critique of Previous Attempt:\n",
    "    {critic_response}\n",
    "    \n",
    "    Please try again to answer the original question, fixing the issues mentioned above.\n",
    "    Original Question: {gaia_question}\n",
    "    \"\"\"\n",
    "\n",
    "    final_response = agent_executor.invoke({\"input\": retry_prompt})\n",
    "    final_answer = final_response[\"output\"]\n",
    "    print(f\"\\nFinal Answer:\\n{final_answer}\")\n",
    "# </solution>\n",
    "else:\n",
    "    print(\"\\nFirst attempt passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1312e7a",
   "metadata": {},
   "source": [
    "### Why this matters for GAIA\n",
    "GAIA questions result in low success rates (originally ~7% for GPT-4) because they are brittle.\n",
    "One missed step (e.g. wrong singular/plural form, wrong order) fails the question.\n",
    "Reflexion loops catch these \"silly\" errors by strictly validating against the constraints before submitting."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
