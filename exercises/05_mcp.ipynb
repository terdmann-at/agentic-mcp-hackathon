{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5aff250",
   "metadata": {},
   "source": [
    "# Exercise 5: Intro to MCP\n",
    "\n",
    "In this notebook, we'll define a MCP server and deploy it via Databricks Apps.\n",
    "\n",
    "Then, we will connect to it via a client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925b71c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %mkdir custom_mcp_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bc8292",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%writefile custom_mcp_server/app.py\n",
    "from fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(name=\"MyServer\")\n",
    "\n",
    "\n",
    "@mcp.tool\n",
    "def hello(name: str) -> str:\n",
    "    return f\"Hello, {name}!\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run(transport=\"http\", host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07624b51",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "For the Databricks Apps deployment, we'll need two extra files:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab554a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"custom_mcp_server/app.yaml\", \"w\") as f:\n",
    "    f.write(\"command: ['python', 'main.py']\")\n",
    "\n",
    "with open(\"custom_mcp_server/requirements.txt\", \"w\") as f:\n",
    "    f.write(\"fastmcp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb49552",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "\n",
    "Head to \"Compute\" -> \"Apps\" -> \"Create Custom App\"\n",
    "\n",
    "Now let's define an agent that will use the tools from the MCP server.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9ef5db",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "Now let's define an agent that will use the tools from the MCP server.\n",
    "\n",
    "First we will need some packages again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8822b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langgraph databricks-langchain\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8c1139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Annotated, Any, Optional, Sequence, TypedDict, Union\n",
    "\n",
    "import nest_asyncio\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_langchain import (\n",
    "    DatabricksMCPServer,\n",
    "    DatabricksMultiServerMCPClient,\n",
    ")\n",
    "from langchain.messages import AIMessage, AnyMessage\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt.tool_node import ToolNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17089d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# In order to connect to the MCP server in the Databricks Apps application, we\n",
    "# need to create a service principal. Upon creating we get the client id and secret,\n",
    "# which we will use below.\n",
    "#\n",
    "# The host is just the url you are at currently.\n",
    "\n",
    "custom_mcp_server_workspace_client = WorkspaceClient(\n",
    "    host=\"TODO\",\n",
    "    client_id=\"TODO\",\n",
    "    client_secret=\"TODO\",\n",
    "    auth_type=\"oauth-m2m\",  # Enables service principal authentication\n",
    ")\n",
    "\n",
    "databricks_mcp_client = DatabricksMultiServerMCPClient(\n",
    "    [\n",
    "        # DatabricksMCPServer(\n",
    "        #     name=\"system-ai\",\n",
    "        #     url=f\"{host}/api/2.0/mcp/functions/system/ai\",\n",
    "        # ),\n",
    "        DatabricksMCPServer(\n",
    "            name=\"TODO\",\n",
    "            url=\"TODO\",\n",
    "            workspace_client=custom_mcp_server_workspace_client,\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d79dd6b",
   "metadata": {},
   "source": [
    "\n",
    "Now let's connect the client to an Agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47535120",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "from llm import model as llm\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant that can run Python code.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# The state for the agent workflow, including the conversation and any custom data\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[AnyMessage], add_messages]\n",
    "    custom_inputs: Optional[dict[str, Any]]\n",
    "    custom_outputs: Optional[dict[str, Any]]\n",
    "\n",
    "\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[ToolNode, Sequence[BaseTool]],\n",
    "    system_prompt: Optional[str] = None,\n",
    "):\n",
    "    model = model.bind_tools(tools)  # Bind tools to the model\n",
    "\n",
    "    # Function to check if agent should continue or finish based on last message\n",
    "    def should_continue(state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        # If function (tool) calls are present, continue; otherwise, end\n",
    "        if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "            return \"continue\"\n",
    "        else:\n",
    "            return \"end\"\n",
    "\n",
    "    def clean_content(messages):\n",
    "        for m in messages:\n",
    "            if hasattr(m, \"content\") and isinstance(m.content, list):\n",
    "                for block in m.content:\n",
    "                    if isinstance(block, dict):\n",
    "                        block.pop(\"id\", None)  # Remove the offending field\n",
    "        return messages\n",
    "\n",
    "    # Preprocess: optionally prepend a system prompt to the conversation history\n",
    "    if system_prompt:\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "            + clean_content(state[\"messages\"])\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: clean_content(state[\"messages\"]))\n",
    "\n",
    "    model_runnable = preprocessor | model  # Chain the preprocessor and the model\n",
    "\n",
    "    # The function to invoke the model within the workflow\n",
    "    def call_model(\n",
    "        state: AgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    workflow = StateGraph(AgentState)  # Create the agent's state machine\n",
    "\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))  # Agent node (LLM)\n",
    "    workflow.add_node(\"tools\", ToolNode(tools))  # Tools node\n",
    "\n",
    "    workflow.set_entry_point(\"agent\")  # Start at agent node\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"continue\": \"tools\",  # If the model requests a tool call, move to tools node\n",
    "            \"end\": END,  # Otherwise, end the workflow\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"agent\")  # After tools are called, return to agent node\n",
    "\n",
    "    # Compile and return the tool-calling agent workflow\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "# Initialize the entire agent, including MCP tools and workflow\n",
    "def initialize_agent():\n",
    "    \"\"\"Initialize the agent with MCP tools\"\"\"\n",
    "    # Create MCP tools from the configured servers\n",
    "    mcp_tools = asyncio.run(databricks_mcp_client.get_tools())\n",
    "    print(mcp_tools)\n",
    "    # Create the agent graph with an LLM, tool set, and system prompt (if given)\n",
    "    agent = create_tool_calling_agent(llm, mcp_tools, system_prompt)\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db996e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent()\n",
    "agent.invoke({\"messages:\"[\"what tools you have?\"]})"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
