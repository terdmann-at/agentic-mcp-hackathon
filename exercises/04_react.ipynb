{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8c83c6f",
   "metadata": {},
   "source": [
    "# Exercise 4: ReAct Agent\n",
    "\n",
    "In this file, we'll build a ReAct agent using the LangGraph framework.\n",
    "It would not be difficult to write an ReAct agent without a framework as well.\n",
    "However, LangGraph provides low-level supporting infrastructure for defining agents,\n",
    "which makes some things like persistence, state-management easy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03840522",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install databricks-langchain langchain-community ddgs langgraph\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aea9ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Literal\n",
    "\n",
    "from databricks_langchain import ChatDatabricks\n",
    "from langchain.messages import AnyMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "from langchain.tools import BaseTool, tool\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from typing_extensions import Annotated, TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24360b15",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from llm import model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcb2912",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Tools\n",
    "\n",
    "Let's define a couple tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71891fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply `a` and `b`.Args: a: First int, b: Second int\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds `a` and `b`.Args: a: First int, b: Second int\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "# Exercise 4.1: Define the divide tool\n",
    "# Hint: Use the @tool decorator. The function should take two ints and return a float.\n",
    "# <solution>\n",
    "@tool\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide `a` and `b`.Args: a: First int, b: Second int\"\"\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "# </solution>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2882b75e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "Now we are ready to define the agent. For this we will build a graph,\n",
    "consisting of Nodes such as the agent node (where we invoke the LLM), or\n",
    "a tool node (where we invoke the tools as requested by the LLM).\n",
    "\n",
    "Further, we'll define edges (to define the order of execution), and\n",
    "conditional edges (branching points), which allows us to create loops.\n",
    "\n",
    "For the ReAct agent, we'll loop between LLM calls and tool calls. The loop\n",
    "ends when the LLM does not emit any tool calls (and thus has provided the final answer).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae30de58",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# the agent will have these tools\n",
    "tools = [add, multiply, divide]\n",
    "\n",
    "\n",
    "def build_agent(tools: list[BaseTool]):\n",
    "    tools_by_name = {tool.name: tool for tool in tools}\n",
    "    model_with_tools = model.bind_tools(tools)\n",
    "\n",
    "    # State\n",
    "    class MessagesState(TypedDict):\n",
    "        messages: Annotated[list[AnyMessage], operator.add]\n",
    "        llm_calls: int\n",
    "\n",
    "    # Nodes\n",
    "    def llm_call(state: dict):\n",
    "        \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                model_with_tools.invoke(\n",
    "                    [\n",
    "                        SystemMessage(\n",
    "                            content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
    "                        )\n",
    "                    ]\n",
    "                    + state[\"messages\"]\n",
    "                )\n",
    "            ],\n",
    "            \"llm_calls\": state.get(\"llm_calls\", 0) + 1,\n",
    "        }\n",
    "\n",
    "    def tool_node(state: dict):\n",
    "        \"\"\"Performs the tool call\"\"\"\n",
    "        result = []\n",
    "        for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "            # Exercise 4.2: Handle the tool call. That is, call the function with the arguments\n",
    "            # requested by the LLM and add the result (as a `ToolMessage`) to the conversation history.\n",
    "            # <solution>\n",
    "            tool = tools_by_name[tool_call[\"name\"]]\n",
    "            observation = tool.invoke(tool_call[\"args\"])\n",
    "            result.append(\n",
    "                ToolMessage(content=observation, tool_call_id=tool_call[\"id\"])\n",
    "            )\n",
    "            # </solution>\n",
    "        return {\"messages\": result}\n",
    "\n",
    "    # Exercise 4.3: Define conditional logic\n",
    "    # Hint: Check if the last message in state[\"messages\"] has `tool_calls`.\n",
    "    def should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\n",
    "        \"\"\"Decide if we should continue the loop or stop\"\"\"\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        # <solution>\n",
    "        if last_message.tool_calls:\n",
    "            return \"tool_node\"\n",
    "        return END\n",
    "        # </solution>\n",
    "\n",
    "    # Exercise 4.4: Initialize graph and add nodes\n",
    "    # Hint: Use StateGraph(MessagesState). Add nodes using .add_node(\"name\", function).\n",
    "    # <solution>\n",
    "    agent_builder = StateGraph(MessagesState)\n",
    "    agent_builder.add_node(\"llm_call\", llm_call)\n",
    "    agent_builder.add_node(\"tool_node\", tool_node)\n",
    "    # </solution>\n",
    "\n",
    "    # Exercise 4.5: Add edges\n",
    "    # Hint: Use .add_edge(start, end) and .add_conditional_edges(source, condition, path_map).\n",
    "    # Connect START -> llm_call, then conditionally to tool_node or END, and tool_node back to llm_call.\n",
    "    # <solution>\n",
    "    agent_builder.add_edge(START, \"llm_call\")\n",
    "    agent_builder.add_conditional_edges(\"llm_call\", should_continue, [\"tool_node\", END])\n",
    "    agent_builder.add_edge(\"tool_node\", \"llm_call\")\n",
    "    # </solution>\n",
    "\n",
    "    return agent_builder.compile()\n",
    "\n",
    "\n",
    "# Exercise 4.6: Compile the graph\n",
    "# Hint: Call .compile() on the graph builder.\n",
    "# <solution>\n",
    "agent = build_agent(tools)\n",
    "# </solution>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f089c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Let's run this query\n",
    "math_query = \"Calculate ((144 / 12) * (25 + 75)) / ((10 * 10) / (500 / 5)) + ((81 / 9) * (121 / 11))\"\n",
    "\n",
    "# Exercise 4.7: Invoke the agent.\n",
    "# Hint: Initialize the conversation history with the query wrapped in a `HumanMessage` and use agent.invoke({\"messages\": messages})\n",
    "# <solution>\n",
    "messages = [HumanMessage(content=math_query)]\n",
    "response = agent.invoke({\"messages\": messages})\n",
    "# </solution>\n",
    "\n",
    "# show the message history\n",
    "for m in response[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd202509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.8: Create another agent, with the ability to search the web.\n",
    "#\n",
    "from ddgs import DDGS\n",
    "\n",
    "\n",
    "# Exercise 4.9: Use the above to create a search tool\n",
    "# <solution>\n",
    "@tool\n",
    "def web_search(query: str, max_results: int = 5):\n",
    "    \"\"\"Run a web search\"\"\"\n",
    "    return str(DDGS().text(query, max_results=max_results))\n",
    "\n",
    "\n",
    "# </solution>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9032e5a1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "search_query = \"What is Altana?\"\n",
    "# Exercise 4.9: Build and invoke the agent.\n",
    "# <solution>\n",
    "agent = build_agent([web_search])\n",
    "messages = [HumanMessage(content=search_query)]\n",
    "response = agent.invoke({\"messages\": messages})\n",
    "# </solution>\n",
    "\n",
    "for m in response[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050b3c32",
   "metadata": {},
   "source": [
    "## Demo: Functional API\n",
    "\n",
    "LangGraph also supports a functional API that relies on decorators.\n",
    "This can be more intuitive for python developers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a796e7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def build_agent_func(tools):\n",
    "    # Augment the LLM with tools\n",
    "    tools_by_name = {tool.name: tool for tool in tools}\n",
    "    model_with_tools = model.bind_tools(tools)\n",
    "\n",
    "    from langchain.messages import (\n",
    "        ToolCall,\n",
    "    )\n",
    "    from langchain_core.messages import BaseMessage\n",
    "    from langgraph.func import entrypoint, task\n",
    "    from langgraph.graph import add_messages\n",
    "\n",
    "    # Step 2: Define model node\n",
    "    @task\n",
    "    def call_llm(messages: list[BaseMessage]):\n",
    "        \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
    "        return model_with_tools.invoke(\n",
    "            [\n",
    "                SystemMessage(\n",
    "                    content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
    "                )\n",
    "            ]\n",
    "            + messages\n",
    "        )\n",
    "\n",
    "    # Step 3: Define tool node\n",
    "    @task\n",
    "    def call_tool(tool_call: ToolCall):\n",
    "        \"\"\"Performs the tool call\"\"\"\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        return tool.invoke(tool_call)\n",
    "\n",
    "    # Step 4: Define agent\n",
    "    @entrypoint(checkpointer=checkpointer)\n",
    "    def agent(messages: list[BaseMessage]):\n",
    "        model_response = call_llm(messages).result()\n",
    "\n",
    "        while True:\n",
    "            if not model_response.tool_calls:\n",
    "                break\n",
    "\n",
    "            # Execute tools\n",
    "            tool_result_futures = [\n",
    "                call_tool(tool_call) for tool_call in model_response.tool_calls\n",
    "            ]\n",
    "            tool_results = [fut.result() for fut in tool_result_futures]\n",
    "            messages = add_messages(messages, [model_response, *tool_results])\n",
    "            model_response = call_llm(messages).result()\n",
    "\n",
    "        messages = add_messages(messages, model_response)\n",
    "        return messages\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c04dd0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "agent = build_agent_func([add, multiply, divide])\n",
    "# Invoke\n",
    "for chunk in agent.stream(messages, stream_mode=\"updates\"):\n",
    "    print(chunk)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210677c8",
   "metadata": {},
   "source": [
    "# Exercise 4.10 (Bonus): Turn the agent into a chatbot.\n",
    "\n",
    "Hint: We need a checkpointer: https://docs.langchain.com/oss/python/langgraph/persistence#checkpoints\n",
    "\n",
    "<solution>\n",
    "from langchain.checkpointers.memory import InMemorySaver\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "agent = create_agent(tools, checkpointer=checkpointer)\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    response = agent.invoke({\n",
    "        \"messages\": [HumanMessage(content=user_input)]}, \n",
    "        config={\"configurable\": {\"thread_id\": \"1\"}})\n",
    "    response[\"messages\"][-1].pretty_print()\n",
    "</solution>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
