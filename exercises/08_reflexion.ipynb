{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d4441fd",
   "metadata": {},
   "source": [
    "# Exercise 8: Reflexion Pattern\n",
    "\n",
    "Goal: Implement the **Reflexion** design pattern.\n",
    "\n",
    "Reflexion uses a \"critique\" step to verify the agent's output and provide feedback for a revision.\n",
    "This simple loop often improves performance on complex reasoning tasks significantly.\n",
    "\n",
    "Pattern: `Actor` -> `Evaluate/Critique` -> `Self-Reflection` -> `Repeat` (if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9d9c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install databricks-langchain\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44871bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "model = ChatDatabricks(endpoint=\"databricks-claude-sonnet-4-5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377223e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First Attempt Generation\n",
    "# Let's give it a tricky task.\n",
    "\n",
    "task_prompt = \"\"\"\n",
    "Write a python function that calculates the Levenshtein distance between two strings \n",
    "WITHOUT importing any libraries.\n",
    "\"\"\"\n",
    "\n",
    "# Initial Actor\n",
    "actor = model\n",
    "initial_solution = actor.invoke(task_prompt).content\n",
    "print(\"--- Initial Solution ---\")\n",
    "print(initial_solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c599dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8.1: Critiques / Reflection\n",
    "# Now we ask the model to critique its own code.\n",
    "# Hint: Create a prompt asking the model to review the code.\n",
    "\n",
    "# <solution>\n",
    "reflection_prompt = f\"\"\"\n",
    "You are a senior software engineer. Review the following python code for correctness, efficiency, and style.\n",
    "If there are errors, describe them clearly. If it is perfect, say \"PERFECT\".\n",
    "\n",
    "Code:\n",
    "{initial_solution}\n",
    "\"\"\"\n",
    "\n",
    "critique = model.invoke(reflection_prompt).content\n",
    "print(\"\\n--- Critique ---\")\n",
    "print(critique)\n",
    "# </solution>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdce9ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8.2: Revision\n",
    "# We pass the original request, the initial solution, and the critique back to the actor.\n",
    "# Hint: If the critique is not \"PERFECT\", ask the model to rewrite the code.\n",
    "\n",
    "# <solution>\n",
    "if \"PERFECT\" not in critique:\n",
    "    revision_prompt = f\"\"\"\n",
    "    The user asked for: {task_prompt}\n",
    "\n",
    "    You provided this solution:\n",
    "    {initial_solution}\n",
    "\n",
    "    The reviewer gave this feedback:\n",
    "    {critique}\n",
    "\n",
    "    Please rewrite the code to address the feedback.\n",
    "    \"\"\"\n",
    "\n",
    "    final_solution = model.invoke(revision_prompt).content\n",
    "    print(\"\\n--- Final Solution ---\")\n",
    "    print(final_solution)\n",
    "else:\n",
    "    print(\"\\nNo revision needed!\")\n",
    "# </solution>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b046aee",
   "metadata": {},
   "source": [
    "### Advanced Implementation\n",
    "In a real system (like using LangGraph), you would loop this cycle until the critique passed or a max_steps limit was reached.\n",
    "This technique is used in \"Language Models can Solve Computer Tasks\" (RCI) and many coding agents."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
