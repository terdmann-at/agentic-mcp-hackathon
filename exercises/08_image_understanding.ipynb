{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf7cf12",
   "metadata": {},
   "source": [
    "# Exercise 8: Image Understanding\n",
    "\n",
    "First, we'll need to install a couple packages. If using serverless, add the packages `langchain` and `databricks-langchain` to the environment.\n",
    "If not using serverless run the below cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57710fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain databricks-langchain\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8adb12",
   "metadata": {},
   "source": [
    "We'll be working with Databricks LLMs that support vision capabilities.\n",
    "Below we see an example for how to instantiate the model and invoke it with an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe695088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import urllib.request\n",
    "\n",
    "from databricks_langchain import ChatDatabricks\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "model = ChatDatabricks(endpoint=\"databricks-gemma-3-12b\")\n",
    "\n",
    "# Helper to encode image from local path\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "# Helper to encode image from URL\n",
    "def encode_image_from_url(url):\n",
    "    req = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        return base64.b64encode(response.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "# Example usage with a URL\n",
    "image_url = \"https://raw.githubusercontent.com/pytorch/vision/main/gallery/assets/dog1.jpg\"\n",
    "image_data = encode_image_from_url(image_url)\n",
    "\n",
    "response = model.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=[\n",
    "                {\"type\": \"text\", \"text\": \"Describe this image detailedly:\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b6a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8.1:\n",
    "#\n",
    "# Build a simple chatbot that can understand images using langchain.\n",
    "# The user should provide an image URL or path at the beginning.\n",
    "#\n",
    "\n",
    "def chat_shell_vision():\n",
    "    # Initialize chat history\n",
    "    chat_history: list[HumanMessage | AIMessage] = []\n",
    "\n",
    "    print(\"Welcome to the Vision Chatbot!\")\n",
    "    image_input = input(\"Please provide an image URL or local file path: \")\n",
    "\n",
    "    start_message_content = []\n",
    "\n",
    "    try:\n",
    "        # Exercise 8.1.1: Request processing of the image\n",
    "        # <solution>\n",
    "        if image_input.startswith(\"http\"):\n",
    "             base64_image = encode_image_from_url(image_input)\n",
    "        else:\n",
    "             base64_image = encode_image(image_input)\n",
    "\n",
    "        start_message_content.append(\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n",
    "            }\n",
    "        )\n",
    "        # </solution>\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image: {e}\")\n",
    "        return\n",
    "\n",
    "    # Add initial instruction\n",
    "    start_message_content.insert(\n",
    "        0, {\"type\": \"text\", \"text\": \"I will ask you questions about this image.\"}\n",
    "    )\n",
    "    chat_history.append(HumanMessage(content=start_message_content))\n",
    "\n",
    "    # Get initial response/acknowledgement\n",
    "    response = model.invoke(chat_history)\n",
    "    print(f\"AI: {response.content}\")\n",
    "    chat_history.append(response)\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "\n",
    "        # Exercise 8.1.3: Add user message and invoke model\n",
    "        # <solution>\n",
    "        chat_history.append(HumanMessage(content=user_input))\n",
    "        response = model.invoke(chat_history)\n",
    "        print(f\"AI: {response.content}\")\n",
    "        chat_history.append(response)\n",
    "        # </solution>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e67d48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the chatbot\n",
    "chat_shell_vision()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
