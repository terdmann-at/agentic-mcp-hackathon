{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be922d38",
   "metadata": {},
   "source": [
    "# Exercise 3: Tool Calling\n",
    "\n",
    "Goal: Use LangChain to call a custom tool and handle the response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231b33ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain databricks-langchain\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e4e826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks_langchain import ChatDatabricks\n",
    "from langchain.messages import HumanMessage, ToolMessage\n",
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee47c80f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "model = ChatDatabricks(endpoint=\"databricks-claude-sonnet-4-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff8d3fd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Tools\n",
    "\n",
    "Tools are simply python functions that are wrapped by the `@tool` decorator.\n",
    "\n",
    "The decorator takes care of translating the docstring and the types of the inputs into instructions\n",
    "for the LLM when given the tool. The LLM does not call the function itself, instead it responds\n",
    "with a json structure representing a call to the function, which we will"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dad669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.1: Define a tool using the decorator\n",
    "# <solution>\n",
    "@tool\n",
    "# </solution>\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get the weather for a location.\"\"\"\n",
    "    if \"Berlin\" in location:\n",
    "        return \"Cloudy, 15C\"\n",
    "    return \"Sunny, 25C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e0ab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    tools = [get_weather]\n",
    "    tools_by_name = {t.name: t for t in tools}\n",
    "\n",
    "    # Exercise 3.2: Bind the tool to the model. Use the .bind_tools method.\n",
    "    # <solution>\n",
    "    model_with_tools = model.bind_tools(tools)\n",
    "    # </solution>\n",
    "\n",
    "    query = \"What is the weather in Berlin?\"\n",
    "    messages = [HumanMessage(content=query)]\n",
    "\n",
    "    # Exercise 3.3: Invoke the model to get the tool call\n",
    "    # <solution>\n",
    "    ai_msg = model_with_tools.invoke(messages)\n",
    "    # </solution>\n",
    "    messages.append(ai_msg)\n",
    "    print(f\"AI Call: {ai_msg.tool_calls}\")\n",
    "\n",
    "    # Exercise 3.4: Execute tool calls manually and append results\n",
    "    for tool_call in ai_msg.tool_calls:\n",
    "        selected_tool = tools_by_name[tool_call[\"name\"]]\n",
    "\n",
    "        # Invoke the selected tool\n",
    "        # <solution>\n",
    "        tool_output = selected_tool.invoke(tool_call[\"args\"])\n",
    "        # </solution>\n",
    "        print(f\"Tool Output: {tool_output}\")\n",
    "\n",
    "        messages.append(ToolMessage(content=tool_output, tool_call_id=tool_call[\"id\"]))\n",
    "\n",
    "    # Exercise 3.5: Get the final response\n",
    "    # <solution>\n",
    "    final_response = model_with_tools.invoke(messages)\n",
    "    # </solution>\n",
    "    print(f\"Final Answer: {final_response.content}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
