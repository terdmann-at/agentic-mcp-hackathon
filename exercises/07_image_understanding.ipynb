{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d740f58d",
   "metadata": {},
   "source": [
    "# Image Understanding Chatbot\n",
    "\n",
    "First, we'll need to install a couple packages. If using serverless, add the packages `langchain` and `databricks-langchain` to the environment.\n",
    "If not using serverless run the below cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0b5bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain databricks-langchain\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22abc387",
   "metadata": {},
   "source": [
    "Today we'll be working with Databricks LLMs that support vision capabilities.\n",
    "Below we see an example for how to instantiate the model and invoke it with an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d26007",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "from databricks_langchain import ChatDatabricks\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "model = ChatDatabricks(endpoint=\"databricks-claude-sonnet-4-5\")\n",
    "\n",
    "# Helper to encode image from local path\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Example usage with a URL\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
    "response = model.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=[\n",
    "                {\"type\": \"text\", \"text\": \"Describe this image detailedly:\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": image_url},\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19f1bf7",
   "metadata": {},
   "source": [
    "Now it's your turn. Solve the exercises below.\n",
    "\n",
    "To test your chatbot, run this on the terminal:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df7ac5a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1c3ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7.1:\n",
    "#\n",
    "# Build a simple chatbot that can understand images using langchain.\n",
    "# The user should provide an image URL or path at the beginning.\n",
    "#\n",
    "# Fill in the lines inside <solution></solution>.\n",
    "#\n",
    "\n",
    "def chat_shell_vision():\n",
    "    # Initialize chat history\n",
    "    chat_history: list[HumanMessage | AIMessage] = []\n",
    "    \n",
    "    print(\"Welcome to the Vision Chatbot!\")\n",
    "    image_input = input(\"Please provide an image URL or local file path: \")\n",
    "\n",
    "    start_message_content = []\n",
    "    \n",
    "    # Simple check if it's a URL or local file\n",
    "    if image_input.startswith(\"http\"):\n",
    "        # <solution>\n",
    "        start_message_content.append({\"type\": \"image_url\", \"image_url\": {\"url\": image_input}})\n",
    "        # </solution>\n",
    "    else:\n",
    "        try:\n",
    "            # <solution>\n",
    "            base64_image = encode_image(image_input)\n",
    "            start_message_content.append(\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n",
    "                }\n",
    "            )\n",
    "            # </solution>\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image: {e}\")\n",
    "            return\n",
    "\n",
    "    # Add initial instruction\n",
    "    start_message_content.insert(0, {\"type\": \"text\", \"text\": \"I will ask you questions about this image.\"})\n",
    "    chat_history.append(HumanMessage(content=start_message_content))\n",
    "    \n",
    "    # Get initial response/acknowledgement\n",
    "    response = model.invoke(chat_history)\n",
    "    print(f\"AI: {response.content}\")\n",
    "    chat_history.append(response)\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "\n",
    "        # Exercise 7.1.1: Add user message and invoke model\n",
    "        # <solution>\n",
    "        chat_history.append(HumanMessage(content=user_input))\n",
    "        response = model.invoke(chat_history)\n",
    "        print(f\"AI: {response.content}\")\n",
    "        chat_history.append(response)\n",
    "        # </solution>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daa49d9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Run the chatbot\n",
    "# chat_shell_vision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7529dc3c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45a01603",
   "metadata": {},
   "source": [
    "## Exercise 7.2 (Bonus):\n",
    "Use streamlit to build a chat interface with image upload.\n",
    "\n",
    "To test it, deploy the app.py to databricks apps using the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddee9450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %mkdir streamlit_app_01_vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02dc728",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile streamlit_app_01_vision/requirements.txt\n",
    "# databricks-langchain\n",
    "# langchain\n",
    "# streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e7aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile streamlit_app_01_vision/app.yaml\n",
    "# command: [\"streamlit\", \"run\", \"app.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c6765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile streamlit_app_01_vision/app.py\n",
    "import streamlit as st\n",
    "from databricks_langchain import ChatDatabricks\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "import base64\n",
    "\n",
    "st.title(\"Vision Chatbot\")\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Sidebar for image upload\n",
    "with st.sidebar:\n",
    "    uploaded_file = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
    "    image_url = st.text_input(\"Or enter Image URL\")\n",
    "\n",
    "# Handle image input\n",
    "current_image_content = None\n",
    "if uploaded_file:\n",
    "    # Display the uploaded image\n",
    "    st.image(uploaded_file, caption=\"Uploaded Image\", use_container_width=True)\n",
    "    bytes_data = uploaded_file.getvalue()\n",
    "    base64_image = base64.b64encode(bytes_data).decode('utf-8')\n",
    "    current_image_content = {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}\n",
    "    }\n",
    "elif image_url:\n",
    "    st.image(image_url, caption=\"Image from URL\", use_container_width=True)\n",
    "    current_image_content = {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\"url\": image_url}\n",
    "    }\n",
    "\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(\"user\" if isinstance(message, HumanMessage) else \"assistant\"):\n",
    "        if isinstance(message.content, list):\n",
    "             # Handle multimodal content display if needed, currently just showing text part if present?\n",
    "             # For simplicity, we might just show the text part in the chat history for now\n",
    "             for block in message.content:\n",
    "                 if isinstance(block, dict) and block.get(\"type\") == \"text\":\n",
    "                     st.markdown(block[\"text\"])\n",
    "        else:\n",
    "            st.markdown(message.content)\n",
    "\n",
    "if prompt := st.chat_input(\"Ask something about the image\"):\n",
    "    \n",
    "    # Construct message content\n",
    "    message_content = []\n",
    "    \n",
    "    # If this is the FIRST message and we have an image, attach it\n",
    "    # Ideally we'd want to attach it contextually, but simple approach:\n",
    "    # If there are no previous messages, attach image. \n",
    "    # Or if the user just uploaded it. \n",
    "    # Let's attach it to the current message if it hasn't been \"sent\" yet.\n",
    "    # A simple way to track if image is sent is to check history length or a flag.\n",
    "    \n",
    "    # Better approach for this demo: Always attach the image if present to *this* prompt \n",
    "    # to ensure the model sees it, or key off session state.\n",
    "    # However, sending image every time consumes tokens. \n",
    "    # Let's send it only once at the start of conversation or if it changes (advanced).\n",
    "    # Simple approach: If history is empty, attach image.\n",
    "    \n",
    "    has_image_in_history = any(\n",
    "        isinstance(m, HumanMessage) and isinstance(m.content, list) and any(b.get(\"type\") == \"image_url\" for b in m.content)\n",
    "        for m in st.session_state.messages\n",
    "    )\n",
    "    \n",
    "    if not has_image_in_history and current_image_content:\n",
    "        message_content.append({\"type\": \"text\", \"text\": prompt})\n",
    "        message_content.append(current_image_content)\n",
    "    else:\n",
    "        message_content = prompt # Just text\n",
    "\n",
    "    st.session_state.messages.append(HumanMessage(content=message_content))\n",
    "    \n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        model = ChatDatabricks(endpoint=\"databricks-claude-sonnet-4-5\")\n",
    "        \n",
    "        # Invoke model\n",
    "        response = model.invoke(st.session_state.messages)\n",
    "        \n",
    "        st.markdown(response.content)\n",
    "        st.session_state.messages.append(response)\n",
    "# </solution>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
